{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook For Web Scraping\n"
      ],
      "metadata": {
        "id": "lL_U3U9Ej2su"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS8PqKfO8Apo",
        "outputId": "99603de2-f73e-4ded-b46d-375a01636fbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.31.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.4.0)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Collecting python-dotenv (from webdriver-manager)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.4.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.16.0)\n",
            "Downloading selenium-4.31.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, python-dotenv, outcome, webdriver-manager, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 python-dotenv-1.1.0 selenium-4.31.0 trio-0.30.0 trio-websocket-0.12.2 webdriver-manager-4.0.2 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium pandas tqdm webdriver-manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BD_RjMzBC1iF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzj8umVE98VD",
        "outputId": "a62b3fb6-abe9-4106-8d09-aaf5c5cb04a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,642 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,718 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,907 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,244 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,544 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,420 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,200 kB]\n",
            "Fetched 24.1 MB in 5s (5,129 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 libudev1 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 snapd\n",
            "  squashfs-tools systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 8 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 30.3 MB of archives.\n",
            "After this operation, 123 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.15 [76.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.15 [1,557 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.67.1+22.04 [27.8 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 30.3 MB in 3s (11.5 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 126101 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.15_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.15) over (249.11-0ubuntu3.12) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.15) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 126301 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.15_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.15) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.67.1+22.04_amd64.deb ...\n",
            "Unpacking snapd (2.67.1+22.04) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.15) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.67.1+22.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 126530 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.15) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMf4oDdl9_SN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ed4XLQ788L-_",
        "outputId": "0ad30587-999d-4e39-c34a-e01b25dde634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rScraping MATLAB docs:   0%|          | 0/500 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshooting-basics.html\n",
            "Successfully scraped: Troubleshooting Basics - MATLAB &amp; Simulink...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   0%|          | 1/500 [00:06<57:29,  6.91s/it, URL=ug/troubleshooting-basics.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 41 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshooting-basics.html#skip_link_anchor\n",
            "Successfully scraped: Troubleshooting Basics - MATLAB &amp; Simulink...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   0%|          | 2/500 [00:11<46:18,  5.58s/it, URL=g-basics.html#skip_link_anchor]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 new links\n",
            "Visiting: https://in.mathworks.com/help/?s_tid=user_nav_help\n",
            "Successfully scraped: Help Center...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   1%|          | 3/500 [00:21<1:03:36,  7.68s/it, URL=.com/help/?s_tid=user_nav_help]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 201 new links\n",
            "Visiting: https://in.mathworks.com/help/index.html?s_tid=CRUX_lftnav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   1%|          | 4/500 [00:26<52:52,  6.40s/it, URL=p/index.html?s_tid=CRUX_lftnav]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully scraped: in.mathworks.com...\n",
            "Found 0 new links\n",
            "Visiting: https://in.mathworks.com/help/overview/real-time-simulation-and-testing.html?s_tid=hc_product_group_bc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   1%|          | 5/500 [00:29<43:16,  5.25s/it, URL=html?s_tid=hc_product_group_bc]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully scraped: in.mathworks.com...\n",
            "Found 0 new links\n",
            "Progress saved: 5 documents\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/index.html?s_tid=CRUX_lftnav\n",
            "Successfully scraped: Simulink Real-Time Documentation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   1%|          | 6/500 [00:36<49:02,  5.96s/it, URL=e/index.html?s_tid=CRUX_lftnav]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 29 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/troubleshooting-in-slrt-target.html?s_tid=CRUX_lftnav\n",
            "Successfully scraped: Troubleshooting in Simulink Real-Time - MATLAB &am...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   1%|▏         | 7/500 [00:45<56:17,  6.85s/it, URL=-target.html?s_tid=CRUX_lftnav]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshooting-basics.html#responsive_offcanvas\n",
            "Successfully scraped: Troubleshooting Basics - MATLAB &amp; Simulink...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   2%|▏         | 8/500 [00:51<55:21,  6.75s/it, URL=sics.html#responsive_offcanvas]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/troubleshooting-in-slrt-target.html?s_tid=CRUX_topnav\n",
            "Successfully scraped: Troubleshooting in Simulink Real-Time - MATLAB &am...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   2%|▏         | 9/500 [00:59<56:39,  6.92s/it, URL=-target.html?s_tid=CRUX_topnav]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/examples.html?s_tid=CRUX_topnav&category=troubleshooting-in-slrt-target\n",
            "Successfully scraped: Example List - MATLAB &amp; Simulink...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   2%|▏         | 10/500 [01:06<58:27,  7.16s/it, URL=troubleshooting-in-slrt-target]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3 new links\n",
            "Progress saved: 10 documents\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/referencelist.html?type=function&s_tid=CRUX_topnav&category=troubleshooting-in-slrt-target\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   2%|▏         | 11/500 [01:11<50:44,  6.23s/it, URL=troubleshooting-in-slrt-target]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully scraped: in.mathworks.com...\n",
            "Found 0 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/referencelist.html?type=block&s_tid=CRUX_topnav&category=troubleshooting-in-slrt-target\n",
            "Successfully scraped: Troubleshooting in Simulink Real-Time — Blocks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   2%|▏         | 12/500 [01:20<58:27,  7.19s/it, URL=troubleshooting-in-slrt-target]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/referencelist.html?type=app&s_tid=CRUX_topnav&category=troubleshooting-in-slrt-target\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   3%|▎         | 13/500 [01:24<50:34,  6.23s/it, URL=troubleshooting-in-slrt-target]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully scraped: in.mathworks.com...\n",
            "Found 0 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshoot-communication-failure-through-firewall.html\n",
            "Successfully scraped: Troubleshoot Communication Failure Through Firewal...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   3%|▎         | 14/500 [01:35<1:03:20,  7.82s/it, URL=-failure-through-firewall.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 13 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshoot-cannot-load-shared-object-on-target-computer.html\n",
            "Successfully scraped: Troubleshoot Cannot Load Shared Object on Target C...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   3%|▎         | 15/500 [01:42<59:48,  7.40s/it, URL=object-on-target-computer.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12 new links\n",
            "Progress saved: 15 documents\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/vector-canape-troubleshooting.html\n",
            "Successfully scraped: Troubleshoot Vector CANape Operation - MATLAB &amp...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   3%|▎         | 16/500 [01:50<1:01:55,  7.68s/it, URL=or-canape-troubleshooting.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 14 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/etas-inca-troubleshooting.html\n",
            "Successfully scraped: Troubleshoot ETAS Inca Operation - MATLAB &amp; Si...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   3%|▎         | 17/500 [01:58<1:00:54,  7.57s/it, URL=etas-inca-troubleshooting.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 13 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug_upgrade/troubleshoot-system-upgrade-to-r2020b.html\n",
            "Successfully scraped: Troubleshoot System Upgrade for R2020b - MATLAB &a...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   4%|▎         | 18/500 [02:04<58:46,  7.32s/it, URL=-system-upgrade-to-r2020b.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshoot-missing-real-time-tab.html\n",
            "Successfully scraped: Troubleshoot Missing Real-Time Tab - MATLAB &amp; ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   4%|▍         | 19/500 [02:11<57:05,  7.12s/it, URL=oot-missing-real-time-tab.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshoot-folder-names-with-spaces-or-special-characters-halt-model-builds.html\n",
            "Successfully scraped: Troubleshoot Folder Names with Spaces or Special C...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   4%|▍         | 20/500 [02:18<55:42,  6.96s/it, URL=racters-halt-model-builds.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8 new links\n",
            "Progress saved: 20 documents\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshoot-model-links-to-static-libraries-or-shared-objects.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   4%|▍         | 21/500 [02:21<46:56,  5.88s/it, URL=braries-or-shared-objects.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully scraped: in.mathworks.com...\n",
            "Found 0 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshoot-build-error-for-accelerator-mode.html\n",
            "Successfully scraped: Troubleshoot Build Error for Accelerator Mode - MA...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   4%|▍         | 22/500 [02:28<50:39,  6.36s/it, URL=rror-for-accelerator-mode.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshoot-long-build-times-for-real-time-application.html\n",
            "Successfully scraped: Troubleshoot Long Build Times for Real-Time Applic...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   5%|▍         | 23/500 [02:34<48:33,  6.11s/it, URL=for-real-time-application.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshoot-working-with-persistent-variables.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   5%|▍         | 24/500 [02:38<42:52,  5.40s/it, URL=with-persistent-variables.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully scraped: in.mathworks.com...\n",
            "Found 0 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug_upgrade/troubleshoot-model-upgrade-to-r2020b.html\n",
            "Successfully scraped: Troubleshoot Model Upgrade for R2020b - MATLAB &am...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   5%|▌         | 25/500 [02:46<48:42,  6.15s/it, URL=t-model-upgrade-to-r2020b.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7 new links\n",
            "Progress saved: 25 documents\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug_upgrade/troubleshoot-s-function-build-upgrade-for-r2020b.html\n",
            "Successfully scraped: Troubleshoot S-Function Build Upgrade for R2020b -...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   5%|▌         | 26/500 [02:53<50:42,  6.42s/it, URL=-build-upgrade-for-r2020b.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshoot-parameters-not-accessible-by-name.html\n",
            "Successfully scraped: Troubleshoot Parameters Not Accessible by Name - M...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   5%|▌         | 27/500 [03:00<51:58,  6.59s/it, URL=rs-not-accessible-by-name.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 11 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshoot-signals-not-accessible-by-name.html\n",
            "Successfully scraped: Troubleshoot Signals Not Accessible by Name - MATL...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   6%|▌         | 28/500 [03:09<57:59,  7.37s/it, URL=ls-not-accessible-by-name.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 16 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshoot-signal-data-logging-from-nonvirtual-bus-fixed-point-and-multidimensional-signals.html\n",
            "Successfully scraped: Troubleshoot Signal Data Logging from Nonvirtual B...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   6%|▌         | 29/500 [03:17<59:16,  7.55s/it, URL=-multidimensional-signals.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 31 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshoot-signal-data-logging-from-inport-ref-model.html\n",
            "Successfully scraped: Troubleshoot Signal Data Logging from Inport in Re...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   6%|▌         | 30/500 [03:26<1:03:59,  8.17s/it, URL=ing-from-inport-ref-model.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 new links\n",
            "Progress saved: 30 documents\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshoot-signal-data-logging-from-inport-ref-mode-test-harness.html\n",
            "Successfully scraped: Troubleshoot Signal Data Logging from Inport in Re...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   6%|▌         | 31/500 [03:33<1:00:13,  7.70s/it, URL=ort-ref-mode-test-harness.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshoot-signal-data-logging-from-send-and-receive-blocks.html\n",
            "Successfully scraped: Troubleshoot Signal Data Logging from Send and Rec...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   6%|▋         | 32/500 [03:41<1:00:05,  7.70s/it, URL=m-send-and-receive-blocks.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshoot-signals-for-streaming-or-file-log-logging.html\n",
            "Successfully scraped: Troubleshoot Signals for Streaming or File Logging...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   7%|▋         | 33/500 [03:47<56:08,  7.21s/it, URL=aming-or-file-log-logging.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshoot-unsatisfactory-real-time-performance.html\n",
            "Successfully scraped: Troubleshoot Unsatisfactory Real-Time Performance ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   7%|▋         | 34/500 [03:54<56:26,  7.27s/it, URL=ory-real-time-performance.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 29 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshoot-overloaded-cpu-from-executing-real-time-application.html\n",
            "Successfully scraped: Troubleshoot Overloaded CPU from Executing Real-Ti...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   7%|▋         | 35/500 [04:03<59:06,  7.63s/it, URL=ing-real-time-application.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 18 new links\n",
            "Progress saved: 35 documents\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug/troubleshoot-gaps-in-streamed-data.html\n",
            "Successfully scraped: Troubleshoot Gaps in Streamed Data - MATLAB &amp; ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB docs:   7%|▋         | 36/500 [04:09<55:14,  7.14s/it, URL=oot-gaps-in-streamed-data.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7 new links\n",
            "Visiting: https://in.mathworks.com/help/slrealtime/ug_upgrade/troubleshoot-matlab-api-call-upgrade-for-r2020b.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rScraping MATLAB docs:   7%|▋         | 36/500 [04:15<54:46,  7.08s/it, URL=oot-gaps-in-streamed-data.html]\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a2635603c10>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/180e853e866cd8a33adb8ce982a2462f\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a2635f2fa10>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/180e853e866cd8a33adb8ce982a2462f\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a2635630b50>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/180e853e866cd8a33adb8ce982a2462f\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping interrupted by user\n",
            "Scraped 36 pages. Data saved to matlab_docs/matlab_docs.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "class MATLABDocScraper:\n",
        "    def __init__(self, start_url, delay_range=(2, 5), output_dir=\"matlab_docs\"):\n",
        "        self.start_url = start_url\n",
        "        self.delay_range = delay_range\n",
        "        self.visited_urls = set()\n",
        "        self.to_visit = [start_url]\n",
        "        self.base_domain = urlparse(start_url).netloc\n",
        "        self.output_dir = output_dir\n",
        "        self.docs = []\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        # Setup Chrome options for Colab\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument('--headless')\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "        chrome_options.add_argument('--disable-gpu')\n",
        "        chrome_options.add_argument('--window-size=1920,1080')\n",
        "\n",
        "        # Add realistic user agent\n",
        "        chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\")\n",
        "\n",
        "        # Initialize the Chrome driver with Colab's ChromeDriver path\n",
        "        self.driver = webdriver.Chrome(options=chrome_options)\n",
        "        self.wait = WebDriverWait(self.driver, 15)\n",
        "\n",
        "    def is_valid_url(self, url):\n",
        "        \"\"\"Check if URL is valid and belongs to the same domain\"\"\"\n",
        "        parsed = urlparse(url)\n",
        "        # Focus on help documentation and filter out non-documentation pages\n",
        "        if parsed.netloc != self.base_domain:\n",
        "            return False\n",
        "        if \"javascript:\" in url or url.endswith((\".pdf\", \".zip\", \".jpg\", \".png\", \".gif\")):\n",
        "            return False\n",
        "        # Focus on help pages\n",
        "        if \"mathworks.com/help\" not in url:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def extract_content(self, url):\n",
        "        \"\"\"Extract relevant content from the page\"\"\"\n",
        "        try:\n",
        "            # Wait for main content to load\n",
        "            self.wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
        "\n",
        "            # Extract title\n",
        "            title = self.driver.title\n",
        "\n",
        "            # Try to find the main content area\n",
        "            main_content = None\n",
        "\n",
        "            # Try different possible content container selectors\n",
        "            for selector in [\"div.body_content\", \"div#doc_center_content\", \"div.row-offcanvas\",\n",
        "                          \"section.content_container\", \"div.doc_content_container\"]:\n",
        "                try:\n",
        "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                    if elements:\n",
        "                        main_content = elements[0]\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            # Fallback to body if specific containers aren't found\n",
        "            if not main_content:\n",
        "                main_content = self.driver.find_element(By.TAG_NAME, \"body\")\n",
        "\n",
        "            # Extract text content\n",
        "            text_content = main_content.text\n",
        "            html_content = main_content.get_attribute('outerHTML')\n",
        "\n",
        "            # Store document\n",
        "            self.docs.append({\n",
        "                \"url\": url,\n",
        "                \"title\": title,\n",
        "                \"content\": text_content,\n",
        "                \"html\": html_content\n",
        "            })\n",
        "\n",
        "            # Print success message for debugging\n",
        "            print(f\"Successfully scraped: {title[:50]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting content from {url}: {str(e)}\")\n",
        "\n",
        "    def extract_links(self):\n",
        "        \"\"\"Extract links from the current page\"\"\"\n",
        "        links = []\n",
        "        current_url = self.driver.current_url\n",
        "\n",
        "        try:\n",
        "            # Find all link elements\n",
        "            a_elements = self.driver.find_elements(By.TAG_NAME, \"a\")\n",
        "\n",
        "            for element in a_elements:\n",
        "                try:\n",
        "                    href = element.get_attribute(\"href\")\n",
        "                    if href:\n",
        "                        absolute_link = urljoin(current_url, href)\n",
        "\n",
        "                        # Only add links that are valid and not already visited or in queue\n",
        "                        if (self.is_valid_url(absolute_link) and\n",
        "                            absolute_link not in self.visited_urls and\n",
        "                            absolute_link not in self.to_visit):\n",
        "                            links.append(absolute_link)\n",
        "                except StaleElementReferenceException:\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    pass  # Silently ignore individual link errors\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error finding links on {current_url}: {str(e)}\")\n",
        "\n",
        "        return links\n",
        "\n",
        "    def clean_data(self):\n",
        "        \"\"\"Clean and preprocess the scraped data\"\"\"\n",
        "        cleaned_docs = []\n",
        "\n",
        "        for doc in self.docs:\n",
        "            content = doc[\"content\"]\n",
        "\n",
        "            # Remove excessive whitespace\n",
        "            content = re.sub(r'\\s+', ' ', content).strip()\n",
        "\n",
        "            # Remove common HTML artifacts\n",
        "            content = re.sub(r'[\\n\\r\\t]', ' ', content)\n",
        "\n",
        "            # Store cleaned content\n",
        "            cleaned_docs.append({\n",
        "                \"url\": doc[\"url\"],\n",
        "                \"title\": doc[\"title\"],\n",
        "                \"content\": content\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(cleaned_docs)\n",
        "\n",
        "    def crawl(self, max_pages=200):\n",
        "        \"\"\"Start crawling from the initial URL\"\"\"\n",
        "        count = 0\n",
        "\n",
        "        try:\n",
        "            with tqdm(total=max_pages, desc=\"Scraping MATLAB docs\") as pbar:\n",
        "                while self.to_visit and count < max_pages:\n",
        "                    # Get the next URL to visit\n",
        "                    url = self.to_visit.pop(0)\n",
        "\n",
        "                    # Skip if already visited\n",
        "                    if url in self.visited_urls:\n",
        "                        continue\n",
        "\n",
        "                    # Mark as visited\n",
        "                    self.visited_urls.add(url)\n",
        "\n",
        "                    try:\n",
        "                        # Random delay to avoid detection\n",
        "                        delay = random.uniform(*self.delay_range)\n",
        "                        time.sleep(delay)\n",
        "\n",
        "                        # Print current URL for debugging\n",
        "                        print(f\"Visiting: {url}\")\n",
        "\n",
        "                        # Navigate to the page\n",
        "                        self.driver.get(url)\n",
        "\n",
        "                        # Check if we've been blocked\n",
        "                        if \"Access Denied\" in self.driver.title or \"Forbidden\" in self.driver.title:\n",
        "                            print(f\"Access denied for {url}, possibly blocked\")\n",
        "                            continue\n",
        "\n",
        "                        # Extract content\n",
        "                        self.extract_content(url)\n",
        "\n",
        "                        # Extract links and add to queue\n",
        "                        links = self.extract_links()\n",
        "                        self.to_visit.extend(links)\n",
        "                        print(f\"Found {len(links)} new links\")\n",
        "\n",
        "                        count += 1\n",
        "                        pbar.update(1)\n",
        "                        pbar.set_postfix({\"URL\": url[-30:] if len(url) > 30 else url})\n",
        "\n",
        "                        # Periodically save progress\n",
        "                        if count % 5 == 0:\n",
        "                            temp_df = self.clean_data()\n",
        "                            temp_df.to_csv(f\"{self.output_dir}/matlab_docs_progress.csv\", index=False)\n",
        "                            print(f\"Progress saved: {len(temp_df)} documents\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing {url}: {str(e)}\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"Scraping interrupted by user\")\n",
        "\n",
        "        finally:\n",
        "            # Close the browser\n",
        "            self.driver.quit()\n",
        "\n",
        "        # Save final results as CSV\n",
        "        df = self.clean_data()\n",
        "        if not df.empty:\n",
        "            df.to_csv(f\"{self.output_dir}/matlab_docs.csv\", index=False)\n",
        "            print(f\"Scraped {len(df)} pages. Data saved to {self.output_dir}/matlab_docs.csv\")\n",
        "        else:\n",
        "            print(\"No data was scraped.\")\n",
        "\n",
        "        return df\n",
        "\n",
        "# Start scraping\n",
        "if __name__ == \"__main__\":\n",
        "    start_url = \"https://in.mathworks.com/help/slrealtime/ug/troubleshooting-basics.html\"\n",
        "    scraper = MATLABDocScraper(start_url)\n",
        "    docs_df = scraper.crawl(max_pages=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgW0PMzZ8SXa",
        "outputId": "13b5279a-cb7a-476a-c8b5-f56a3380b65a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB links:   5%|▌         | 10/200 [01:24<22:52,  7.23s/it, URL=html?s_tid=hc_product_group_bc]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress saved: 1929 links collected\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB links:  10%|█         | 20/200 [02:47<31:54, 10.64s/it, URL=arget&page=1&s_tid=CRUX_topnav]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress saved: 3714 links collected\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB links:  15%|█▌        | 30/200 [03:36<12:04,  4.26s/it, URL=for-real-time-application.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress saved: 4381 links collected\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB links:  20%|██        | 40/200 [04:35<15:31,  5.82s/it, URL=aming-or-file-log-logging.html]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress saved: 5285 links collected\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB links:  25%|██▌       | 50/200 [05:40<14:46,  5.91s/it, URL=ments/piracy.html?s_tid=gf_pir]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress saved: 6491 links collected\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB links:  30%|███       | 60/200 [06:59<22:52,  9.80s/it, URL=nal-category:recwebinar&page=1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress saved: 8385 links collected\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB links:  35%|███▌      | 70/200 [08:32<17:14,  7.96s/it, URL=orks.html?s_tid=nav_company_dc]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress saved: 10520 links collected\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB links:  40%|████      | 80/200 [10:06<20:19, 10.16s/it, URL=lligence.html?s_tid=hp_hero_ai]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress saved: 12992 links collected\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB links:  45%|████▌     | 90/200 [11:16<13:18,  7.26s/it, URL=ml?s_tid=hp_solutions_robotics]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress saved: 14371 links collected\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB links:  50%|█████     | 100/200 [12:27<12:11,  7.31s/it, URL=ml?s_tid=hp_teaching_resources]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress saved: 15763 links collected\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping MATLAB links:  52%|█████▏    | 103/200 [12:59<12:13,  7.56s/it, URL=t_sales.html?s_tid=hp_trial_cs]\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a26350195d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/8aa71d2a9e52f77645bd66884eaf6e88\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a2635299cd0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/8aa71d2a9e52f77645bd66884eaf6e88\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a2634fd26d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/8aa71d2a9e52f77645bd66884eaf6e88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping interrupted by user\n",
            "Scraped 16347 links from 103 pages. Data saved to matlab_links/matlab_links.csv\n",
            "\n",
            "Sample of scraped links:\n",
            "                                          source_url  \\\n",
            "0  https://in.mathworks.com/help/slrealtime/ug/tr...   \n",
            "1  https://in.mathworks.com/help/slrealtime/ug/tr...   \n",
            "2  https://in.mathworks.com/help/slrealtime/ug/tr...   \n",
            "3  https://in.mathworks.com/help/slrealtime/ug/tr...   \n",
            "4  https://in.mathworks.com/help/slrealtime/ug/tr...   \n",
            "\n",
            "                                     source_title  \\\n",
            "0  Troubleshooting Basics - MATLAB &amp; Simulink   \n",
            "1  Troubleshooting Basics - MATLAB &amp; Simulink   \n",
            "2  Troubleshooting Basics - MATLAB &amp; Simulink   \n",
            "3  Troubleshooting Basics - MATLAB &amp; Simulink   \n",
            "4  Troubleshooting Basics - MATLAB &amp; Simulink   \n",
            "\n",
            "                                            link_url           link_text  \\\n",
            "0  https://in.mathworks.com/help/slrealtime/ug/tr...     Skip to content   \n",
            "1      https://in.mathworks.com/?s_tid=user_nav_logo                       \n",
            "2  https://in.mathworks.com/help/?s_tid=user_nav_...  MATLAB Help Center   \n",
            "3  https://in.mathworks.com/help/slrealtime/ug/tr...                       \n",
            "4  https://in.mathworks.com/help/slrealtime/ug/tr...                       \n",
            "\n",
            "   link_type  \n",
            "0  hyperlink  \n",
            "1  hyperlink  \n",
            "2  hyperlink  \n",
            "3  hyperlink  \n",
            "4  hyperlink  \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import StaleElementReferenceException\n",
        "\n",
        "class MATLABLinkScraper:\n",
        "    def __init__(self, start_url, delay_range=(1.5, 3), output_dir=\"matlab_links\"):\n",
        "        self.start_url = start_url\n",
        "        self.delay_range = delay_range\n",
        "        self.visited_urls = set()\n",
        "        self.to_visit = [start_url]\n",
        "        self.base_domain = urlparse(start_url).netloc\n",
        "        self.output_dir = output_dir\n",
        "        self.links_data = []  # Will store all links found\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        # Setup Chrome options for Colab\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument('--headless')\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "        chrome_options.add_argument('--disable-gpu')\n",
        "\n",
        "        # Add realistic user agent\n",
        "        chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\")\n",
        "\n",
        "        # Initialize the Chrome driver\n",
        "        self.driver = webdriver.Chrome(options=chrome_options)\n",
        "        self.wait = WebDriverWait(self.driver, 10)\n",
        "\n",
        "    def is_valid_url(self, url):\n",
        "        \"\"\"Check if URL is valid and belongs to the same domain\"\"\"\n",
        "        if not url:\n",
        "            return False\n",
        "        parsed = urlparse(url)\n",
        "        # Focus on mathworks domain\n",
        "        if parsed.netloc != self.base_domain:\n",
        "            return False\n",
        "        if \"javascript:\" in url:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def extract_all_links(self):\n",
        "        \"\"\"Extract all links (a href and img src) from the current page\"\"\"\n",
        "        current_url = self.driver.current_url\n",
        "        page_links = []\n",
        "        source_page_title = self.driver.title\n",
        "\n",
        "        try:\n",
        "            # Extract regular hyperlinks\n",
        "            a_elements = self.driver.find_elements(By.TAG_NAME, \"a\")\n",
        "            for element in a_elements:\n",
        "                try:\n",
        "                    href = element.get_attribute(\"href\")\n",
        "                    text = element.text\n",
        "                    if href:\n",
        "                        absolute_link = urljoin(current_url, href)\n",
        "                        link_type = \"hyperlink\"\n",
        "\n",
        "                        # Add to links data\n",
        "                        self.links_data.append({\n",
        "                            \"source_url\": current_url,\n",
        "                            \"source_title\": source_page_title,\n",
        "                            \"link_url\": absolute_link,\n",
        "                            \"link_text\": text,\n",
        "                            \"link_type\": link_type\n",
        "                        })\n",
        "\n",
        "                        # Only add valid links to visit queue\n",
        "                        if (self.is_valid_url(absolute_link) and\n",
        "                            absolute_link not in self.visited_urls and\n",
        "                            absolute_link not in self.to_visit):\n",
        "                            page_links.append(absolute_link)\n",
        "\n",
        "                except StaleElementReferenceException:\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    pass\n",
        "\n",
        "            # Extract image links\n",
        "            img_elements = self.driver.find_elements(By.TAG_NAME, \"img\")\n",
        "            for element in img_elements:\n",
        "                try:\n",
        "                    src = element.get_attribute(\"src\")\n",
        "                    alt = element.get_attribute(\"alt\") or \"\"\n",
        "                    if src:\n",
        "                        absolute_link = urljoin(current_url, src)\n",
        "                        link_type = \"image\"\n",
        "\n",
        "                        # Add to links data\n",
        "                        self.links_data.append({\n",
        "                            \"source_url\": current_url,\n",
        "                            \"source_title\": source_page_title,\n",
        "                            \"link_url\": absolute_link,\n",
        "                            \"link_text\": alt,\n",
        "                            \"link_type\": link_type\n",
        "                        })\n",
        "\n",
        "                except StaleElementReferenceException:\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    pass\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error finding links on {current_url}: {str(e)}\")\n",
        "\n",
        "        return page_links\n",
        "\n",
        "    def save_links_to_csv(self):\n",
        "        \"\"\"Save collected links to CSV file\"\"\"\n",
        "        df = pd.DataFrame(self.links_data)\n",
        "        csv_path = f\"{self.output_dir}/matlab_links.csv\"\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        return df\n",
        "\n",
        "    def crawl(self, max_pages=200):\n",
        "        \"\"\"Start crawling from the initial URL\"\"\"\n",
        "        count = 0\n",
        "\n",
        "        try:\n",
        "            with tqdm(total=max_pages, desc=\"Scraping MATLAB links\") as pbar:\n",
        "                while self.to_visit and count < max_pages:\n",
        "                    # Get the next URL to visit\n",
        "                    url = self.to_visit.pop(0)\n",
        "\n",
        "                    # Skip if already visited\n",
        "                    if url in self.visited_urls:\n",
        "                        continue\n",
        "\n",
        "                    # Mark as visited\n",
        "                    self.visited_urls.add(url)\n",
        "\n",
        "                    try:\n",
        "                        # Random delay to avoid detection\n",
        "                        delay = random.uniform(*self.delay_range)\n",
        "                        time.sleep(delay)\n",
        "\n",
        "                        # Navigate to the page\n",
        "                        self.driver.get(url)\n",
        "\n",
        "                        # Check if we've been blocked\n",
        "                        if \"Access Denied\" in self.driver.title or \"Forbidden\" in self.driver.title:\n",
        "                            print(f\"Access denied for {url}, possibly blocked\")\n",
        "                            continue\n",
        "\n",
        "                        # Extract links and add to queue\n",
        "                        links = self.extract_all_links()\n",
        "                        self.to_visit.extend(links)\n",
        "\n",
        "                        count += 1\n",
        "                        pbar.update(1)\n",
        "                        pbar.set_postfix({\"URL\": url[-30:] if len(url) > 30 else url})\n",
        "\n",
        "                        # Periodically save progress\n",
        "                        if count % 10 == 0:\n",
        "                            temp_df = self.save_links_to_csv()\n",
        "                            print(f\"Progress saved: {len(temp_df)} links collected\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing {url}: {str(e)}\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"Scraping interrupted by user\")\n",
        "\n",
        "        finally:\n",
        "            # Close the browser\n",
        "            self.driver.quit()\n",
        "\n",
        "        # Save final results as CSV\n",
        "        final_df = self.save_links_to_csv()\n",
        "        print(f\"Scraped {len(final_df)} links from {count} pages. Data saved to {self.output_dir}/matlab_links.csv\")\n",
        "\n",
        "        return final_df\n",
        "\n",
        "# Start scraping\n",
        "if __name__ == \"__main__\":\n",
        "    start_url = \"https://in.mathworks.com/help/slrealtime/ug/troubleshooting-basics.html\"\n",
        "    scraper = MATLABLinkScraper(start_url)\n",
        "    links_df = scraper.crawl(max_pages=200)  # Adjust as needed\n",
        "\n",
        "    # Display sample of scraped links\n",
        "    print(\"\\nSample of scraped links:\")\n",
        "    print(links_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying out CrawlForAI"
      ],
      "metadata": {
        "id": "kJQ4usfpjzzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n",
        "from crawl4ai.deep_crawling import BFSDeepCrawlStrategy\n",
        "from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy\n",
        "\n",
        "async def main():\n",
        "    # Configure a 2-level deep crawl\n",
        "    config = CrawlerRunConfig(\n",
        "        deep_crawl_strategy=BFSDeepCrawlStrategy(\n",
        "            max_depth=2,\n",
        "            include_external=False\n",
        "        ),\n",
        "        scraping_strategy=LXMLWebScrapingStrategy(),\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        results = await crawler.arun(\"https://example.com\", config=config)\n",
        "\n",
        "        print(f\"Crawled {len(results)} pages in total\")\n",
        "\n",
        "        # Access individual results\n",
        "        for result in results[:3]:  # Show first 3 results\n",
        "            print(f\"URL: {result.url}\")\n",
        "            print(f\"Depth: {result.metadata.get('depth', 0)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n"
      ],
      "metadata": {
        "id": "jIT9mGwtQaHT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}